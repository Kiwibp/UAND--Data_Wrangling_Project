{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were three different data sets that I had to wrangle.  The first was an archive of tweets from weratedogs twitter handle.  This was simple and straight forward to extract because all I needed to do was download the flat file that Udacity provided and read it into a dataframe using pandas like so: \n",
    "```python\n",
    "twitter_archive_df = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dataset needed to be extracted using the requests library.  This was also simple and straightforward because Udacity provided the url which had the flat file I needed and then I simply used the following command to extract the raw data:\n",
    "```python\n",
    "image_predictions = \n",
    "    re.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')\n",
    "```\n",
    "\n",
    "Then I needed to take the content of the raw data (which was in type *Bytes*) and convert it into a datframe using the following code:\n",
    "```python\n",
    "image_predictions_df = pd.read_csv(io.StringIO(rawData.decode('utf-8')), sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third dataset was the most difficult to wrangle.  It was the first time I used an API wrapper.  I first needed to get my creditials from https://apps.twitter.com and after I submitted my request and received the credentials I used the following code to extract the raw data I needed:\n",
    "```python\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "#json parser will output tweet info in dictionary format\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, \n",
    "      parser=tweepy.parsers.JSONParser())\n",
    "```\n",
    "The api variable is a special object type from tweepy library and extracts the tweets in a Json format.  Since Json objects are structured very similar to to a python dictionary it was easy to read the raw data using the \"keys\" in the key-value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest hurdle I ran into was writing the tweets data into a txt file.  Because this block of code takes a while to run, I first created a list of four tweet id's and did a test run before I started to go through each tweet id in the twitter archive dataframe.\n",
    "```python\n",
    "with open('tweet_json.txt', 'a+', encoding='utf-8') as file:\n",
    "    for tweet_id in twitter_archive_df['tweet_id']:\n",
    "        try:\n",
    "            tweet = api.get_status(id = tweet_id, tweet_mode='extended')\n",
    "            file.write(json.dumps(tweet))\n",
    "            file.write('\\n')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "file.close()\n",
    "```\n",
    "\n",
    "I then read the Json objects into a dataframe using the following code:\n",
    "```python\n",
    "with open('tweet_json.txt') as file:\n",
    "    status = []\n",
    "    for line in file:\n",
    "        status.append(json.loads(line))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a combination of programmatic and visual assessments to determine a number of messy and tidy data issues that I would need to clean.\n",
    "\n",
    "I primarily used the following commands in pandas to do my assessments:\n",
    "```python\n",
    "df.head()\n",
    "df.tail()\n",
    "df.info()\n",
    "df.sample()\n",
    "df.describe()\n",
    "df.value_counts()\n",
    "```\n",
    "\n",
    "I also used the following commands to find duplicated values and the amount of null values for certain columns:\n",
    "```python\n",
    "df[df['column'].duplicated()]\n",
    "\n",
    "len(df[pd.isnull(df['column'])])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final stage of the data wrangling process I worked on the fixing the issues I found in the assessment stage.  My revisions were primarily made to the twitter archive dataframe and the tweets dataframe I extracted using the requests library.  To ensure that I didn't screw up my original dataframes, I made copies of both using the following code:\n",
    "```python\n",
    "twitter_archive_df_clean = twitter_archive_df.copy()\n",
    "tweets_df_clean = tweets_df.copy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I used the pandas library to make adjustments to the dataframes and then combine the dataframes into one master dataframe.  This is where the pandas library really shines and is extremely powerful.  I love this library and plan on improving my knowledge of it as I progress my career in data analytics.  After I cleaned all the issues I found in the assessment stage, I merged the dataframes into one master dataframe using the following code:\n",
    "```python\n",
    "tweets_master_df = pd.merge(twitter_archive_df_clean, tweets_df_clean, \n",
    "                            on='tweet_id', how='right')\n",
    "\n",
    "tweets_master_df = pd.merge(tweets_master_df, image_predictions_df, \n",
    "                            on='tweet_id', how='right')\n",
    "```\n",
    "\n",
    "Then I dropped any rows which still had remaining dirty data (there was only a handful) using the following code:\n",
    "```python\n",
    "tweets_master_df.dropna(inplace=True)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
